{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demo-title",
   "metadata": {},
   "source": [
    "# Individual Fairness Analysis - Demonstration\n",
    "\n",
    "This notebook demonstrates how to perform comprehensive individual fairness analysis using the fairness evaluation framework. It shows how to:\n",
    "\n",
    "1. **Load and prepare your data** for fairness analysis\n",
    "2. **Calculate distance-based fairness metrics** across different feature sets\n",
    "3. **Analyze fairness by demographic groups** with heatmap visualizations\n",
    "4. **Generate Mean Prediction Probability Difference (MPPD)** plots\n",
    "\n",
    "**To run the code**: Replace the data loading sections below with your own dataset following the structure requirements described in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-src",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fairness analysis functions\n",
    "from src.util import TestPredictions\n",
    "from src.constants import ATTRIBUTES_REVERSE_MAPPINGS\n",
    "from src.individual import (\n",
    "    calculate_distance_based_fairness,\n",
    "    cosine_distance,\n",
    "    analyze_individual_fairness_by_group,\n",
    "    filter_fairness_results,\n",
    "    plot_fairness_heatmap,\n",
    "    plot_fairness_comparison,\n",
    "    plot_multiple_group_comparison_proba_diff,\n",
    "    save_fairness_results,\n",
    "    load_fairness_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "**To use**: Replace this section with your own data loading code. You need:\n",
    "- `X_test`: DataFrame with features including sensitive attributes\n",
    "- `y_test`: Binary ground truth labels  \n",
    "- `y_pred_proba`: Model probability predictions\n",
    "- `demographic_mappings`: Mappings from codes to group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-demo-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO DATA LOADING - Replace with your own data loading code\n",
    "# This section shows the expected data structure\n",
    "\n",
    "# Example: Load your trained model predictions\n",
    "# delirium_pred = TestPredictions.load('../output/delirium_xgb_model.pkl')\n",
    "# X_test = delirium_pred.X_test\n",
    "# y_test = delirium_pred.y_test  \n",
    "# y_pred_proba = delirium_pred.y_probs\n",
    "\n",
    "# For demonstration purposes, create sample data structure:\n",
    "print(\"For this demo, you would load your data here.\")\n",
    "print(\"Expected data structure:\")\n",
    "print(\"- X_test: DataFrame with features + sensitive attributes\")\n",
    "print(\"- y_test: Binary labels (0/1)\")\n",
    "print(\"- y_pred_proba: Probability predictions (0-1)\")\n",
    "\n",
    "# Other ways to load data:\n",
    "# UNCOMMENT AND MODIFY FOR YOUR DATA:\n",
    "# X_test = pd.read_csv('your_features.csv')  # Must include demographic columns\n",
    "# y_test = np.load('your_labels.npy')        # Binary outcomes\n",
    "# y_pred_proba = np.load('your_preds.npy')  # Model probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-mappings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic mappings for your data\n",
    "# MODIFY these mappings to match your data encoding\n",
    "\n",
    "demographic_mappings = {\n",
    "    'race_ethnicity': ATTRIBUTES_REVERSE_MAPPINGS['race_ethnicity'],\n",
    "    'sex': ATTRIBUTES_REVERSE_MAPPINGS['sex'],\n",
    "    'insurance_type': ATTRIBUTES_REVERSE_MAPPINGS['insurance_type']\n",
    "}\n",
    "\n",
    "print(\"Demographic group mappings:\")\n",
    "for attr, mapping in demographic_mappings.items():\n",
    "    print(f\"{attr}: {mapping}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-demographic-analysis",
   "metadata": {},
   "source": [
    "## 3. Demographic Group Fairness Analysis\n",
    "\n",
    "This section analyzes individual fairness across different demographic groups and creates comparison visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-by-demographics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze fairness across all demographic groups\n",
    "def analyze_fairness_across_demographics(\n",
    "    X_test,\n",
    "    y_pred_proba,\n",
    "    feature_set='all_clinical',\n",
    "    distance_threshold=0.01,\n",
    "    max_samples=10000,\n",
    "    attribute_mappings=None\n",
    "):\n",
    "    \"\"\"Analyze individual fairness across all demographic attributes\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for demo_key, _ in attribute_mappings.items():\n",
    "        print(f\"\\nAnalyzing individual fairness for {demo_key}...\")\n",
    "        \n",
    "        results[demo_key] = analyze_individual_fairness_by_group(\n",
    "            X_test=X_test,\n",
    "            y_pred_proba=y_pred_proba,\n",
    "            group_column=demo_key,\n",
    "            feature_set=feature_set,\n",
    "            distance_threshold=distance_threshold,\n",
    "            max_samples=max_samples,\n",
    "            attribute_mappings=attribute_mappings,\n",
    "            process_specific_group=None\n",
    "        )\n",
    "        \n",
    "        print(f\"Completed analysis for {demo_key}\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"Demographic group analysis function defined.\")\n",
    "print(\"This will analyze fairness separately for each demographic group.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-demographic-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Run demographic analysis\n",
    "# UNCOMMENT FOR ACTUAL ANALYSIS:\n",
    "# results = analyze_fairness_across_demographics(\n",
    "#     X_test=X_test,\n",
    "#     y_pred_proba=y_pred_proba,\n",
    "#     feature_set='all_clinical',\n",
    "#     distance_threshold=0.01,\n",
    "#     max_samples=10000,\n",
    "#     attribute_mappings=demographic_mappings\n",
    "# )\n",
    "\n",
    "print(\"Demographic group analysis would run here.\")\n",
    "print(\"This analyzes fairness within and across different demographic groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-heatmaps",
   "metadata": {},
   "source": [
    "## 4. Fairness Heatmap Visualizations\n",
    "\n",
    "Create heatmap visualizations showing prediction differences between demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out unknown groups and extract results by demographic attribute\n",
    "# UNCOMMENT FOR ACTUAL FILTERING:\n",
    "# race_result_df = filter_fairness_results(\n",
    "#     results['race_ethnicity'], \n",
    "#     exclude_groups=['Unknown or declined to state']\n",
    "# )\n",
    "# \n",
    "# sex_result_df = results['sex']\n",
    "# \n",
    "# insurance_result_df = filter_fairness_results(\n",
    "#     results['insurance_type'], \n",
    "#     exclude_groups=['Unknown']\n",
    "# )\n",
    "\n",
    "print(\"Results filtering would happen here.\")\n",
    "print(\"This removes groups with insufficient data or unknown categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-race-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fairness heatmap for race/ethnicity\n",
    "# UNCOMMENT FOR ACTUAL PLOTTING:\n",
    "# plot_fairness_heatmap(\n",
    "#     race_result_df,\n",
    "#     title=\"Model Fairness Analysis - Race/Ethnicity Groups\",\n",
    "#     figsize=(10, 8),\n",
    "#     annot=True,\n",
    "#     vmin=0.03,\n",
    "#     vmax=0.125,\n",
    "#     savefig=False\n",
    "# )\n",
    "\n",
    "print(\"Race/ethnicity fairness heatmap would appear here.\")\n",
    "print(\"Darker colors indicate larger prediction differences between groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-mppd",
   "metadata": {},
   "source": [
    "## 5. Mean Prediction Probability Difference (MPPD) Analysis\n",
    "\n",
    "MPPD measures the difference in predictions between clinically similar patients from different demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mppd-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze specific demographic groups for MPPD\n",
    "# UNCOMMENT FOR ACTUAL ANALYSIS:\n",
    "# # Focus on specific reference groups for clearer analysis\n",
    "# race_mppd_df = analyze_individual_fairness_by_group(\n",
    "#     X_test, \n",
    "#     y_pred_proba, \n",
    "#     feature_set='all_clinical',\n",
    "#     distance_threshold=0.01,\n",
    "#     max_samples=10000,\n",
    "#     group_column='race_ethnicity',\n",
    "#     attribute_mappings=ATTRIBUTES_REVERSE_MAPPINGS,\n",
    "#     process_specific_group='White'  # Use White as reference group\n",
    "# )\n",
    "#\n",
    "# sex_mppd_df = analyze_individual_fairness_by_group(\n",
    "#     X_test, \n",
    "#     y_pred_proba,\n",
    "#     feature_set='all_clinical', \n",
    "#     distance_threshold=0.01,\n",
    "#     max_samples=10000,\n",
    "#     group_column='sex',\n",
    "#     attribute_mappings=ATTRIBUTES_REVERSE_MAPPINGS,\n",
    "#     process_specific_group='Female'  # Use Female as reference group\n",
    "# )\n",
    "#\n",
    "# insurance_mppd_df = analyze_individual_fairness_by_group(\n",
    "#     X_test, \n",
    "#     y_pred_proba,\n",
    "#     feature_set='all_clinical',\n",
    "#     distance_threshold=0.01,\n",
    "#     max_samples=10000,\n",
    "#     group_column='insurance_type',\n",
    "#     attribute_mappings=ATTRIBUTES_REVERSE_MAPPINGS,\n",
    "#     process_specific_group='Private'  # Use Private as reference group\n",
    "# )\n",
    "#\n",
    "# # Filter unknown groups\n",
    "# race_mppd_df = filter_fairness_results(race_mppd_df, exclude_groups=['Unknown or declined to state'])\n",
    "# insurance_mppd_df = filter_fairness_results(insurance_mppd_df, exclude_groups=['Unknown'])\n",
    "\n",
    "print(\"MPPD analysis would run here.\")\n",
    "print(\"This focuses on specific reference groups for cleaner comparisons.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-mppd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MPPD comparison plot\n",
    "# UNCOMMENT FOR ACTUAL PLOTTING:\n",
    "# plot_multiple_group_comparison_proba_diff(\n",
    "#     [race_mppd_df, sex_mppd_df, insurance_mppd_df],\n",
    "#     sensitive_attributes=['Race/Ethnicity', 'Sex', 'Insurance Type'],\n",
    "#     title='Mean Prediction Probability Differences of Clinically Similar Patients\\nby Sensitive Attribute Groups',\n",
    "#     figsize=(16, 10),\n",
    "#     save_path=None  # Set to file path to save\n",
    "# )\n",
    "\n",
    "print(\"MPPD comparison plot would appear here.\")\n",
    "print(\"This shows prediction differences between clinically similar patients from different groups.\")\n",
    "print(\"Larger differences indicate potential bias in the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-interpretation",
   "metadata": {},
   "source": [
    "## 6. Results Interpretation Guide\n",
    "\n",
    "### Understanding Individual Fairness Metrics:\n",
    "\n",
    "**Fairness Heatmaps:**\n",
    "- **Darker colors = Larger prediction differences** between groups\n",
    "- Diagonal elements show within-group fairness\n",
    "- Off-diagonal elements show between-group differences\n",
    "\n",
    "**MPPD (Mean Prediction Probability Difference):**\n",
    "- Measures prediction differences between clinically similar patients from different demographic groups\n",
    "- **Higher MPPD values indicate potential bias**\n",
    "- The Î” (delta) values show the range of prediction differences within each sensitive attribute\n",
    "\n",
    "### Next Steps for Your Analysis:\n",
    "\n",
    "1. **Replace the demo data** with your trained model's predictions\n",
    "2. **Update demographic mappings** to match your data encoding \n",
    "3. **Run the full analysis** by uncommenting the analysis code blocks\n",
    "4. **Interpret results** in the context of your specific use case\n",
    "5. **Consider model improvements** if significant fairness issues are detected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-save-results",
   "metadata": {},
   "source": [
    "## 7. Save Results (Optional)\n",
    "\n",
    "Save your analysis results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fairness analysis results\n",
    "# UNCOMMENT TO SAVE RESULTS:\n",
    "# save_fairness_results(\n",
    "#     [race_mppd_df, sex_mppd_df, insurance_mppd_df], \n",
    "#     suffix='_your_model_name'\n",
    "# )\n",
    "\n",
    "print(\"Results saving would happen here.\")\n",
    "print(\"This preserves your analysis for future comparison and reporting.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mppd_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
