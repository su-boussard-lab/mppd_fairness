{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demo-title",
   "metadata": {},
   "source": [
    "# Group Fairness Analysis - Demonstration\n",
    "\n",
    "This notebook demonstrates how to perform comprehensive group fairness analysis using the fairness evaluation framework. It shows how to:\n",
    "\n",
    "1. **Load and prepare your data** for group fairness evaluation\n",
    "2. **Calculate group fairness metrics** across demographic attributes\n",
    "3. **Visualize fairness disparities** using bar charts and radar plots\n",
    "4. **Analyze specific metrics** by demographic groups\n",
    "5. **Interpret results** to identify potential biases\n",
    "\n",
    "**To run the code**: Replace the data loading sections below with your own dataset following the structure requirements described in the README.\n",
    "\n",
    "## Group Fairness Metrics Covered:\n",
    "- **Demographic Parity (DP)**: Equal positive prediction rates across groups\n",
    "- **Equalized Odds (EO)**: Equal true positive and false positive rates across groups\n",
    "- **Equal Opportunity (EOD)**: Equal true positive rates across groups\n",
    "- **Error Rate Disparity Index (EDDI)**: Normalized error rate differences across groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "autoreload",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-fairness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import fairness analysis functions\n",
    "from src.util import TestPredictions\n",
    "from src.constants import ATTRIBUTES_REVERSE_MAPPINGS\n",
    "from src.group import (\n",
    "    calculate_and_visualize_fairness_metrics,\n",
    "    visualize_metric_by_group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "**To use**: Replace this section with your own data loading code. You need:\n",
    "- `X_test`: DataFrame with features including sensitive attributes\n",
    "- `y_test`: Binary ground truth labels (0/1)\n",
    "- `y_pred`: Binary model predictions (0/1) \n",
    "- `demographic_variables`: Dictionary with demographic group assignments\n",
    "- `demographic_mappings`: Mappings from codes to readable group names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-demo-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO DATA LOADING - Replace with your own data loading code\n",
    "# This section shows the expected data structure\n",
    "\n",
    "# Example: Load your trained model predictions\n",
    "# model_pred = TestPredictions.load('../output/delirium_xgb_model.pkl')\n",
    "# y_test = model_pred.y_test      # Ground truth binary labels\n",
    "# y_pred = model_pred.y_pred      # Model binary predictions\n",
    "# X_test = model_pred.X_test      # Feature data with demographic attributes\n",
    "\n",
    "print(\"For this demo, you would load your data here.\")\n",
    "print(\"Expected data structure:\")\n",
    "print(\"- y_test: Binary ground truth labels (0/1)\")\n",
    "print(\"- y_pred: Binary model predictions (0/1)\")\n",
    "print(\"- X_test: DataFrame with demographic attributes\")\n",
    "\n",
    "# Other ways to load data:\n",
    "# UNCOMMENT AND MODIFY FOR YOUR DATA:\n",
    "# y_test = np.load('your_true_labels.npy')     # Binary ground truth\n",
    "# y_pred = np.load('your_predictions.npy')    # Binary predictions (use threshold on probabilities)\n",
    "# X_test = pd.read_csv('your_features.csv')   # Must include demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demographic mappings for your data\n",
    "# MODIFY these mappings to match your data encoding\n",
    "\n",
    "demographic_mappings = {\n",
    "    'race_ethnicity': ATTRIBUTES_REVERSE_MAPPINGS['race_ethnicity'],\n",
    "    'sex': ATTRIBUTES_REVERSE_MAPPINGS['sex'],\n",
    "    'insurance_type': ATTRIBUTES_REVERSE_MAPPINGS['insurance_type']\n",
    "}\n",
    "\n",
    "print(\"Available demographic group mappings:\")\n",
    "for attr, mapping in demographic_mappings.items():\n",
    "    print(f\"\\n{attr.upper()}:\")\n",
    "    for code, label in mapping.items():\n",
    "        print(f\"  {code}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-demographic-vars",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare demographic variables from your data\n",
    "# UNCOMMENT AND MODIFY FOR YOUR DATA:\n",
    "# demographic_variables = {\n",
    "#     'Race_Ethnicity': X_test['race_ethnicity'].astype(int),\n",
    "#     'Sex': X_test['sex'].astype(int),\n",
    "#     'Insurance': X_test['insurance_type'].astype(int)\n",
    "# }\n",
    "\n",
    "print(\"Demographic variables would be extracted here.\")\n",
    "print(\"These define which demographic group each patient belongs to.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-data-filtering",
   "metadata": {},
   "source": [
    "## 3. Data Filtering and Preprocessing\n",
    "\n",
    "Remove samples with unknown demographic information to ensure clean analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-unknown-groups",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out samples with unknown demographic information\n",
    "# UNCOMMENT FOR ACTUAL FILTERING:\n",
    "\n",
    "# # Filter Race/Ethnicity (remove -1 values)\n",
    "# mask_race = demographic_variables['Race_Ethnicity'] != -1\n",
    "# race_ethnicity_clean = demographic_variables['Race_Ethnicity'][mask_race]\n",
    "# y_test_race = y_test[mask_race]\n",
    "# y_pred_race = y_pred[mask_race]\n",
    "# \n",
    "# # Filter Sex (remove -1 values)\n",
    "# mask_sex = demographic_variables['Sex'] != -1\n",
    "# sex_clean = demographic_variables['Sex'][mask_sex]\n",
    "# y_test_sex = y_test[mask_sex]\n",
    "# y_pred_sex = y_pred[mask_sex]\n",
    "# \n",
    "# # Filter Insurance (remove -1 values)\n",
    "# mask_insurance = demographic_variables['Insurance'] != -1\n",
    "# insurance_clean = demographic_variables['Insurance'][mask_insurance]\n",
    "# y_test_insurance = y_test[mask_insurance]\n",
    "# y_pred_insurance = y_pred[mask_insurance]\n",
    "\n",
    "print(\"Data filtering would happen here.\")\n",
    "print(\"This removes samples with unknown (-1) demographic values for cleaner analysis.\")\n",
    "\n",
    "# Display filtering statistics\n",
    "# print(f\"Original dataset size: {len(y_test)}\")\n",
    "# print(f\"Race/Ethnicity analysis: {len(y_test_race)} samples ({len(y_test_race)/len(y_test)*100:.1f}%)\")\n",
    "# print(f\"Sex analysis: {len(y_test_sex)} samples ({len(y_test_sex)/len(y_test)*100:.1f}%)\")\n",
    "# print(f\"Insurance analysis: {len(y_test_insurance)} samples ({len(y_test_insurance)/len(y_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-race-analysis",
   "metadata": {},
   "source": [
    "## 4. Race/Ethnicity Fairness Analysis\n",
    "\n",
    "Analyze fairness across racial and ethnic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-race-fairness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize fairness metrics for Race/Ethnicity\n",
    "# UNCOMMENT FOR ACTUAL ANALYSIS:\n",
    "# race_metrics, race_viz_metrics = calculate_and_visualize_fairness_metrics(\n",
    "#     y_test_race, \n",
    "#     y_pred_race, \n",
    "#     race_ethnicity_clean,\n",
    "#     demographic_mappings['race_ethnicity'],\n",
    "#     title='Model Fairness Metrics by Race and Ethnicity',\n",
    "#     show_plot=\"both\",  # Shows both bar chart and radar chart\n",
    "#     save_pdf=False     # Set to True to save plots\n",
    "# )\n",
    "\n",
    "print(\"Race/ethnicity fairness analysis would run here.\")\n",
    "print(\"This generates comprehensive fairness metrics and visualizations.\")\n",
    "print(\"Results include bar charts and radar plots showing disparity levels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "race-specific-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific fairness metric by race/ethnicity groups\n",
    "# UNCOMMENT FOR SPECIFIC METRIC VISUALIZATION:\n",
    "# # Remove 'Unknown' group if present for cleaner visualization\n",
    "# if 'Unknown or declined to state' in race_metrics[0]:\n",
    "#     del race_metrics[0]['Unknown or declined to state']\n",
    "#\n",
    "# # Visualize Equalized Odds differences\n",
    "# visualize_metric_by_group(\n",
    "#     race_metrics[0], \n",
    "#     'Equalized Odds', \n",
    "#     'Equalized Odds',\n",
    "#     title='Equalized Odds Difference by Race/Ethnicity',\n",
    "#     figsize=(10, 6),\n",
    "#     save_pdf=False\n",
    "# )\n",
    "\n",
    "print(\"Individual metric visualization would appear here.\")\n",
    "print(\"This shows detailed breakdown of a specific fairness metric across groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-sex-analysis",
   "metadata": {},
   "source": [
    "## 5. Sex/Gender Fairness Analysis\n",
    "\n",
    "Analyze fairness across sex/gender groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-sex-fairness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize fairness metrics for Sex\n",
    "# UNCOMMENT FOR ACTUAL ANALYSIS:\n",
    "# sex_metrics, sex_viz_metrics = calculate_and_visualize_fairness_metrics(\n",
    "#     y_test_sex, \n",
    "#     y_pred_sex, \n",
    "#     sex_clean, \n",
    "#     demographic_mappings['sex'],\n",
    "#     title='Model Fairness Metrics by Sex',\n",
    "#     show_plot=\"both\",\n",
    "#     save_pdf=False\n",
    "# )\n",
    "\n",
    "print(\"Sex/gender fairness analysis would run here.\")\n",
    "print(\"This compares fairness metrics between male and female patients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-insurance-analysis",
   "metadata": {},
   "source": [
    "## 6. Insurance Type Fairness Analysis\n",
    "\n",
    "Analyze fairness across insurance/socioeconomic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-insurance-fairness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize fairness metrics for Insurance Type\n",
    "# UNCOMMENT FOR ACTUAL ANALYSIS:\n",
    "# insurance_metrics, insurance_viz_metrics = calculate_and_visualize_fairness_metrics(\n",
    "#     y_test_insurance, \n",
    "#     y_pred_insurance, \n",
    "#     insurance_clean,\n",
    "#     demographic_mappings['insurance_type'],\n",
    "#     title='Model Fairness Metrics by Insurance Type',\n",
    "#     show_plot=\"both\",\n",
    "#     save_pdf=False\n",
    "# )\n",
    "\n",
    "print(\"Insurance type fairness analysis would run here.\")\n",
    "print(\"This examines fairness across different insurance/socioeconomic groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insurance-specific-metric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific metric for insurance groups\n",
    "# UNCOMMENT FOR SPECIFIC METRIC VISUALIZATION:\n",
    "# visualize_metric_by_group(\n",
    "#     insurance_metrics[0], \n",
    "#     'EDDI',           # Error Rate Disparity Index\n",
    "#     'Error Rate',     # Raw metric to display alongside\n",
    "#     title='Error Rate Disparity Index (EDDI) by Insurance Type',\n",
    "#     figsize=(10, 6),\n",
    "#     save_pdf=False\n",
    "# )\n",
    "\n",
    "print(\"EDDI (Error Rate Disparity Index) visualization would appear here.\")\n",
    "print(\"EDDI measures normalized error rate differences across insurance groups.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-interpretation",
   "metadata": {},
   "source": [
    "## 7. Results Interpretation Guide\n",
    "\n",
    "### Understanding Group Fairness Metrics:\n",
    "\n",
    "**Metric Interpretations (Lower values = Better fairness):**\n",
    "\n",
    "1. **Demographic Parity Difference (DPD)**:\n",
    "   - Measures difference in positive prediction rates between groups\n",
    "   - **Ideal value**: 0 (equal positive rates across all groups)\n",
    "   - **Threshold guidance**: <0.05 good, 0.05-0.10 moderate concern, >0.10 high concern\n",
    "\n",
    "2. **Equal Opportunity Difference (EOD)**:\n",
    "   - Measures difference in true positive rates (sensitivity) between groups\n",
    "   - **Ideal value**: 0 (equal benefit for positive cases across groups)\n",
    "   - **Clinical significance**: Ensures equal detection of positive cases\n",
    "\n",
    "3. **Equalized Odds**:\n",
    "   - Combines true positive rate and false positive rate differences\n",
    "   - **Ideal value**: 0 (equal performance across groups)\n",
    "   - **Comprehensive measure**: Considers both benefits and harms\n",
    "\n",
    "4. **Error Rate Disparity Index (EDDI)**:\n",
    "   - Normalized measure of error rate differences\n",
    "   - **Range**: 0 to 1, where 0 = perfect fairness\n",
    "   - **Advantage**: Accounts for baseline error rates\n",
    "\n",
    "### Visualization Guide:\n",
    "\n",
    "**Bar Charts**:\n",
    "- **Green bars**: Low disparity (good fairness)\n",
    "- **Yellow bars**: Moderate disparity (monitor)\n",
    "- **Orange/Red bars**: High disparity (requires attention)\n",
    "\n",
    "**Radar Charts**:\n",
    "- **Center (0)**: Perfect fairness\n",
    "- **Outer rings**: Increasing disparity levels\n",
    "- **Shape**: Balanced polygon indicates consistent fairness across metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mppd_fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
